import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from matplotlib.colors import ListedColormap

def render():
    st.header("üå≥ Cap√≠tulo 3: √Årboles de Decisi√≥n")
    st.markdown("---")

    # ==========================================
    # SECCI√ìN TE√ìRICA (M√≥dulos Markdown)
    # ==========================================
    
    st.markdown("""
    ### 1. ¬øQu√© es un √Årbol de Decisi√≥n?
    Los √°rboles de decisi√≥n son algoritmos de Machine Learning muy vers√°tiles que pueden realizar tareas de clasificaci√≥n, regresi√≥n e incluso tareas de m√∫ltiples salidas. Son modelos muy potentes capaces de ajustarse a conjuntos de datos complejos y constituyen los componentes fundamentales de algoritmos a√∫n m√°s avanzados como los Random Forests (Bosques Aleatorios).

    Se les conoce como modelos de **"caja blanca"** (white box) porque sus decisiones son bastante intuitivas y f√°ciles de interpretar. A diferencia de otros modelos m√°s opacos, los √°rboles de decisi√≥n proporcionan reglas de clasificaci√≥n simples que hasta podr√≠an aplicarse manualmente si fuera necesario.
    """)

    st.markdown("""
    ### 2. Estructura y Funcionamiento
    Para hacer una predicci√≥n, el √°rbol se recorre de arriba hacia abajo:
    * **Nodo ra√≠z:** Es el punto de partida (profundidad 0, en la parte superior) donde se hace la primera pregunta sobre uno de los atributos o caracter√≠sticas del dato.
    * **Nodos intermedios:** Dependiendo de si la respuesta es verdadera o falsa, nos movemos hacia la rama izquierda o derecha, llegando a otros nodos que seguir√°n haciendo preguntas sobre los atributos.
    * **Nodo hoja:** El recorrido termina cuando llegamos a un nodo hoja. Este tipo de nodo no tiene "hijos" (no hace m√°s preguntas) y su funci√≥n es simplemente devolver la clase predicha. Adem√°s, los √°rboles pueden estimar la probabilidad de que una instancia pertenezca a una clase concreta calculando la proporci√≥n de instancias de esa clase presentes en su nodo hoja.
    """)

    st.markdown("""
    ### 3. ¬øC√≥mo se eval√∫a la calidad de una divisi√≥n? (Entrop√≠a y Gini)
    Para elegir el mejor punto de corte en los datos, los algoritmos miden la "impureza" de las particiones creadas:
    * **Impureza Gini:** Un nodo se considera totalmente "puro" (gini = 0) si todas las instancias de entrenamiento que le aplican pertenecen exactamente a la misma clase.
    * **Entrop√≠a y Ganancia de Informaci√≥n:** La entrop√≠a es una medida que nos dice cu√°n lejana est√° una partici√≥n de la perfecci√≥n o la homogeneidad. A mayor entrop√≠a, peor es la partici√≥n. La m√©trica de Ganancia de Informaci√≥n es la diferencia entre la entrop√≠a original y la entrop√≠a tras aplicar el atributo; por lo que el objetivo del algoritmo es maximizar esta ganancia (es decir, minimizar la entrop√≠a).
    """)

    st.markdown("""
    ### 4. Ventajas principales
    Una de las grandes cualidades de los √°rboles de decisi√≥n es que **requieren muy poca preparaci√≥n de los datos**. En particular, no necesitan que realices procesos de escalado o centrado de las caracter√≠sticas (como s√≠ requieren otros algoritmos que hemos visto).
    """)

    st.markdown("---")

    # ==========================================
    # SECCI√ìN INTERACTIVA
    # ==========================================
    st.subheader("üïπÔ∏è Entorno Interactivo")
    
    st.markdown("""
    #### üìä Sobre los datos de este laboratorio
    Para ilustrar c√≥mo funcionan las fronteras de decisi√≥n, estamos utilizando un conjunto de datos sint√©tico cl√°sico llamado **"Make Moons"** (Lunas). 
    Consiste en dos semic√≠rculos de puntos entrelazados (Clase 0 en zonas rojizas y Clase 1 en zonas azuladas). Al a√±adirle "ruido", los puntos se mezclan en el centro, simulando la incertidumbre de un problema real y obligando al √°rbol a esforzarse para separarlos.
    
    *Experimenta con los hiperpar√°metros y observa en la pesta√±a de **Inspecci√≥n de un Punto** c√≥mo el modelo toma decisiones paso a paso.*
    """)

    # Configuraci√≥n de hiperpar√°metros en columnas
    col1, col2 = st.columns([1, 3])

    with col1:
        st.markdown("### ‚öôÔ∏è Hiperpar√°metros")
        
        criterio = st.selectbox(
            "Criterio de Impureza",
            options=["gini", "entropy"]
        )
        
        max_depth = st.slider(
            "Profundidad M√°xima",
            min_value=1, max_value=15, value=3,
            help="Define cu√°ntas preguntas sucesivas puede hacer el √°rbol."
        )
        
        min_samples_leaf = st.slider(
            "Muestras M√≠nimas por Hoja",
            min_value=1, max_value=20, value=1,
            help="Evita que se creen nodos hoja con muy pocos datos."
        )
        
        ruido = st.slider(
            "Ruido en los datos", 
            min_value=0.0, max_value=0.5, value=0.2,
            help="A mayor ruido, m√°s se mezclan las 'lunas'."
        )
                          
        st.markdown("### üé≤ Datos")
        semilla = st.number_input(
            "Semilla Aleatoria (Seed)", 
            min_value=1, max_value=9999, value=42,
            help="Cambia este valor para generar posiciones de puntos completamente nuevas."
        )

    with col2:
        # Generar y dividir datos
        X, y = make_moons(n_samples=300, noise=ruido, random_state=semilla)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=semilla)

        # Entrenar modelo
        clf = DecisionTreeClassifier(
            criterion=criterio,
            max_depth=max_depth,
            min_samples_leaf=min_samples_leaf,
            random_state=semilla
        )
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        
        # Panel de M√©tricas
        st.markdown("#### üìà Rendimiento en Datos Nuevos (Test Set)")
        m1, m2, m3, m4 = st.columns(4)
        m1.metric("Exactitud (Accuracy)", f"{accuracy_score(y_test, y_pred):.2f}")
        m2.metric("Precisi√≥n", f"{precision_score(y_test, y_pred):.2f}")
        m3.metric("Sensibilidad (Recall)", f"{recall_score(y_test, y_pred):.2f}")
        m4.metric("F1-Score", f"{f1_score(y_test, y_pred):.2f}")
        
        with st.expander("‚ÑπÔ∏è ¬øQu√© significan estas m√©tricas?"):
            st.markdown("""
            Estas m√©tricas eval√∫an c√≥mo se comporta el modelo frente a los **datos de prueba**:
            * **Exactitud (Accuracy):** Porcentaje de aciertos totales.
            * **Precisi√≥n:** De los etiquetados como Clase 1, ¬øcu√°ntos eran realmente Clase 1? (Penaliza Falsos Positivos).
            * **Sensibilidad (Recall):** De los que realmente son Clase 1, ¬øcu√°ntos encontr√≥ el modelo? (Penaliza Falsos Negativos).
            * **F1-Score:** Equilibrio entre Precisi√≥n y Sensibilidad.
            """)
        
        st.markdown("<br>", unsafe_allow_html=True)

        # Pesta√±as de visualizaci√≥n: Fronteras y An√°lisis de Camino de Decisi√≥n
        tab_fronteras, tab_inspeccion = st.tabs(["üó∫Ô∏è Fronteras de Decisi√≥n", "üîç Inspecci√≥n de un Punto (Paso a Paso)"])

        with tab_fronteras:
            fig, ax = plt.subplots(figsize=(8, 5))
            
            x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
            y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
            xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
            
            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
            
            cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
            cmap_bold = ListedColormap(['#FF0000', '#0000FF'])
            
            ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)
            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=40, label="Entrenamiento")
            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, edgecolor='black', s=60, marker='*', label="Prueba")
            
            ax.set_title(f"Fronteras (Profundidad = {max_depth})")
            ax.set_xlabel("Caracter√≠stica 1 (Eje X)")
            ax.set_ylabel("Caracter√≠stica 2 (Eje Y)")
            ax.legend(loc="best")
            
            st.pyplot(fig)
            plt.close()

        with tab_inspeccion:
            st.markdown("### üìç Coloca un punto en el mapa")
            st.markdown("Usa los controles para mover el punto amarillo (estrella) y observa c√≥mo el √°rbol decide a qu√© clase pertenece.")
            
            cx1, cx2 = st.columns(2)
            with cx1:
                px = st.slider("Posici√≥n Caracter√≠stica 1 (X)", float(x_min), float(x_max), 0.0)
            with cx2:
                py = st.slider("Posici√≥n Caracter√≠stica 2 (Y)", float(y_min), float(y_max), 0.0)

            # Dibujar el mapa con el punto seleccionado
            fig_punto, ax_punto = plt.subplots(figsize=(8, 3.5))
            ax_punto.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.5) 
            ax_punto.scatter(px, py, c='yellow', edgecolor='black', s=300, marker='*', zorder=5, label="Tu Punto")
            ax_punto.set_xlabel("Caracter√≠stica 1")
            ax_punto.set_ylabel("Caracter√≠stica 2")
            ax_punto.legend(loc="best")
            st.pyplot(fig_punto)
            plt.close()

            # Extraer y mostrar el camino de decisi√≥n
            punto_array = np.array([[px, py]])
            camino = clf.decision_path(punto_array).indices
            
            st.markdown("### üß† L√≥gica del Modelo para este punto:")
            
            for nodo in camino:
                # Si el nodo no es una hoja (tiene hijos)
                if clf.tree_.children_left[nodo] != clf.tree_.children_right[nodo]:
                    atributo = clf.tree_.feature[nodo]
                    umbral = clf.tree_.threshold[nodo]
                    valor_punto = punto_array[0, atributo]
                    
                    nombre_attr = "Caracter√≠stica 1 (X)" if atributo == 0 else "Caracter√≠stica 2 (Y)"
                    
                    if valor_punto <= umbral:
                        st.info(f"**Paso:** ¬øEs {nombre_attr} ({valor_punto:.2f}) $\le$ {umbral:.2f}? **S√≠** ‚û°Ô∏è (Va por la izquierda)")
                    else:
                        st.warning(f"**Paso:** ¬øEs {nombre_attr} ({valor_punto:.2f}) $\le$ {umbral:.2f}? **No** ‚û°Ô∏è (Va por la derecha)")
                else:
                    # Es un nodo hoja
                    prediccion_final = clf.classes_[np.argmax(clf.tree_.value[nodo])]
                    color_clase = "roja" if prediccion_final == 0 else "azul"
                    st.success(f"üéØ **Fin del recorrido:** Llegamos a una hoja. El modelo predice que es de la **Clase {prediccion_final}** (zona {color_clase}).")


    # ==========================================
    # SECCI√ìN DESPLEGABLE (Deep Dive Gini vs Entrop√≠a)
    # ==========================================
    st.markdown("---")
    with st.expander("üî¨ ¬øQuieres saber m√°s sobre la Impureza Gini y la Entrop√≠a?"):
        st.markdown("""
        Tanto **Gini** como **Entrop√≠a** son funciones matem√°ticas que el algoritmo utiliza para evaluar qu√© tan buena es una divisi√≥n. El objetivo del √°rbol es siempre dividir los datos de forma que los nodos resultantes sean lo m√°s "puros" posibles (es decir, que contengan datos de una sola clase).

        #### 1. Impureza Gini
        Mide la probabilidad de clasificar incorrectamente un elemento elegido al azar si lo etiquetamos aleatoriamente seg√∫n la distribuci√≥n de clases en el nodo.
        * **F√≥rmula:** $G = 1 - \sum (p_i)^2$ (donde $p_i$ es la proporci√≥n de la clase $i$ en el nodo).
        * **Rango:** Va de 0 (nodo perfectamente puro) a 0.5 (nodo totalmente mezclado en un problema binario).

        #### 2. Entrop√≠a (Ganancia de Informaci√≥n)
        La entrop√≠a es un concepto que viene de la teor√≠a de la informaci√≥n y mide el nivel de "desorden" o incertidumbre en un nodo.
        * **F√≥rmula:** $H = - \sum p_i \log_2(p_i)$
        * **Rango:** Va de 0 (nodo puro) a 1 (nodo totalmente mezclado en un problema binario).
        """)

    # ==========================================
    # SECCI√ìN DE CONCLUSIONES
    # ==========================================
    st.markdown("---")
    st.header("üìå Conclusiones del Cap√≠tulo")
    
    st.markdown("""
    A trav√©s de este laboratorio interactivo, hemos podido extraer tres grandes lecciones sobre el comportamiento de los √Årboles de Decisi√≥n:

    1. **Son cajas blancas muy intuitivas:** Como has visto en la inspecci√≥n paso a paso, el modelo no hace magia matem√°tica indescifrable; simplemente crea un "embudo" de preguntas de *S√≠ o No* basadas en cortes rectos horizontales y verticales. Esto los hace ideales cuando necesitas explicar y justificar tus predicciones ante usuarios no t√©cnicos.
    2. **El peligro mortal del Sobreajuste (Overfitting):** Si subes la *Profundidad M√°xima* a 10 o 15, ver√°s que el √°rbol empieza a dibujar "islas cuadradas" min√∫sculas para atrapar puntos individuales de ruido. Aunque el modelo parezca perfecto en el mapa de entrenamiento, las m√©tricas en los datos de prueba caer√°n. ¬°Ha memorizado los datos de memoria en lugar de aprender el concepto general!
    3. **Tienen una alta varianza:** Si mantienes todos los par√°metros iguales y solo cambias la *Semilla Aleatoria*, ver√°s c√≥mo las fronteras de decisi√≥n cambian radicalmente. Esto demuestra que los √°rboles de decisi√≥n individuales son muy inestables y sensibles a los datos con los que se entrenan (un problema que se soluciona usando **Bosques Aleatorios**).
    """)
